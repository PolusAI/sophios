# Developer's Guide

## Namespacing

Before outlining the main compilation algorithm, we discuss names and namespacing. To guarantee that input & output variables have unique names when we autogenerate CWL files and graphs, we use namespacing.

For now, each individual Namespace is a string with the following encoding:

`namespace_str = f'{plugin_name}__step__{i + 1}__{step_key}'` i.e. `namespace_str = utils.step_name_str(plugin_name, i+1, step_key)`

`plugin_name` can be either a CWL filename (excluding .cwl), or a yml filename (including .yml)

In the autogenerated CWL files, multiple Namespaces are encoded as a string separated by triple underscores (i.e. `namespaces_str = '___'.join(namespace_strs)`)

so that we can later split the string back into its original Namespace strings (i.e. `namespace_strs = namespaces_str.split('___')`)

In subworkflows, we also discard the *leading* namespaces from the parent workflows (i.e. we use relative namespacing).

This is absolutely critical to ensuring that the CWL files corresponding to Subworkflows are completely independent of their embedding into a parent Workflow. This is one of the main design criteria, so do not mess this up!!! In fact, you can't mess this up because of test_cwl_embedding_independence()

(FYI, we are forced to store this info in specially encoded strings because the CWL schema does not allow us to add extra tags. Specially encoded strings are almost never the right answer, but for now I don't see any other way.)

On the other hand, GraphViz and NetworkX require all names to be globally unique, so by default we do not truncate the namespaces when constructing graphs. However, --graph_inline_depth allows users to hide irrelevant details by collapsing all subgraphs below the given depth to a singe node. This is implemented by simply truncating the *trailing* namespaces. Thus, keeping track of namespaces allows us to trivially implement two key features.

## Compilation Algorithm

One of the main design criteria is that users should be able to (recursively) combine workflow steps into reusable building blocks, i.e. subworkflows. Moreover, everything should be independent of how the user chooses to partition their root workflow into subworkflows. (See test_inline_subworkflows()) To implement this, we use a recursive compilation strategy.

First let's consider the base case, i.e. the case that all of the steps in the workflow are already CWL CommandLineTools and thus there is no recursion. In other words, we have a list of steps with inputs that need to be connected to previous outputs, either explicitly or using inference, which will be discussed below.

Now for the recursive case: If we are in the process of compiling the steps of a workflow and we encounter a subworkflow, we simply compile the subworkflow's yml file contents to CWL, replace the yml file contents in-memory with the compiled CWL, and continue the compilation of the parent workflow as if the subworkflow was already a CWL CommandLineTool.

However, there are two major additional points: After compilation, a namespace is prepended to the input and output variables of a subworkflow to guarantee uniqueness. More importantly, from within the subworkflow, it may not be possible to completely determine all inputs concretely; satisfaction of some inputs may need to be deferred.

### Deferred Satisfaction
When compiling a subworkflow, inputs which originate in a parent workflow (and are thus external to the subworkflow) are not yet in scope. Thus, we cannot yet make an edge (either explicit or inferred) and the inputs cannot yet be concretely satisfied. So we simply create an intermediate input variable in the intermediate subworkflow(s), and as the recursion unwinds there will eventually be a concrete input in some parent workflow. It is very important to note that deferring inputs does NOT affect the DAGs of any of the parent workflows! (Again, see test_inline_subworkflows()) Also note that deferred intermediate inputs will be namespaced accordingly.

### Explicit Edges

Explicit edges are handled first, to prevent edge inference from being applied. Whereas the edge inference algorithm operates 'locally', at one level of recursion at a time, the explicit edge algorithm inherently operates 'globally', requiring deferred information to be passed around through the various levels of recursion. (As such, it was actually much more difficult to implement correctly.)

First, when an edge definition site (&) is encountered, its namespaces are stored in explicit_edge_defs. Then, when an edge call site is encountered (*), we compare the namespaces of the definition and call sites using the [lowest common ancestor](https://en.wikipedia.org/wiki/Lowest_common_ancestor) algorithm to see if they have any leading namespaces in common. If we are already in the common namespace, we can immediately apply the definition information. Otherwise, we need to store the information in explicit_edge_calls to be deferred until the recursion reaches the common namespace. (This is the part that was difficult; please be careful when modifying this code!)

### Edge Inference

The edge inference algorithm is actually rather simple: For each input in the current step of a workflow, it checks for compatible outputs in the previous steps. Since most operations presumably desire the most recent compatible output, by default the steps are checked in reverse order and the outputs of each step are also checked in reverse order. For each output, it first checks for matching types and formats. If there is a unique match, then great! Otherwise, it also checks for matching naming conventions. If there is now a unique match, then great! If there are still multiple matches, it chooses the first (i.e. most recent) match. If there are now no matches, it ignores the naming conventions and chooses the first (i.e. most recent) match based on types and formats only.

Note that there will soon be additional features added to allow users to customize the inference algorithm.

Again note that if we are in a subworkflow, edge inference may fail for some inputs and we may need to defer to a parent workflow.

## Git Submodules

The plugins (i.e. CWL adapters) are added to this repo as git submodules. This is because they should be completely independent of the core inference and compilation algorithms and because they should not need to be modified very often.

As shown in the README, cloning the main repo will not clone the submodules; you will need to run the following command:
```
git submodule init && git submodule update
```

Developers should be very careful when using git submodules! The following links explain why:

* [how do i commit changes in a git submodule](https://stackoverflow.com/questions/5542910/how-do-i-commit-changes-in-a-git-submodule)
* [how to link git repos](https://stackoverflow.com/questions/36554810/how-to-link-folder-from-a-git-repo-to-another-repo)
* [git submodule core concepts](https://www.atlassian.com/git/articles/core-concept-workflows-and-tips)

"Why not use `git subtree`?" I'm not opposed to alternative git workflows, but I think the independent and static nature of the plugins is well-suited to submodules.